{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification trees - Level 1\n",
    "\n",
    "Fill the empty spaces `- - -` with code.\n",
    "Check out the messages after the `#` symbol for extra help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Problem\n",
    "You have been hired by GAP as data analysts! Your first task is to predict how many units of a limited edition jumper will be purchased by the most loyal customers. To do that you and your team have conducted a survey of 701 loyal customers. You and your team collected some valuable data including age, gender, salary, how much money the customer spent that day at the store and the last month, whether the customer has used the online shop, and how many jumpers the customer bought the last year. Some of the customers reply to the last question of the survey to make it clear whether or not they will buy the limited-edition jumper. Unfortunately, the last question was not recorded for all the interviewed people. You want to know how many of the 701 interviewed customers will buy the jumper if more than 70% of the interviewed customers are likely to buy the jumper, then the limited-edition jumper will be launched, but if the percentage is lower, unfortunately, the limited-edition jumper will not see the light. To do that we have to use a classification model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "\n",
    "This notebook uses decision trees to classify and predict whether the age, gender, salary, how much money the customer spent today and the last month in GAP, and how many jumpers the customer bought the last year could predict the new acquisition of a jumper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import the following packages\n",
    "\n",
    "Import `Pandas` as `pd` and `Numpy` as `np` </br>\n",
    "From `sklearn` import `tree` and `metrics` </br>\n",
    "From `sklearn.model_selection` import `train_test_split` </br>\n",
    "Import `seaborn`, and `matplotlib.pyplot` as `sns` and `plt` </br>\n",
    "Import `StringIO` from `sklearn.externals.six` and `Image` from `IPython.display` </br>\n",
    "Import `pydotplus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals.six import StringIO \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "# http://decd.co/classification-link   data from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load data \n",
    "\n",
    "Download the \"WholeDataset.csv\" from [GitHub](https://github.com/DecodedCo/Classification) to your working directory, save it as `WholeDataset.csv`.\n",
    "\n",
    "Import the CSV file into Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read the dataset use the function read_csv from pandas\n",
    "data = pd.read_csv (\"WholeDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the notebook we need to:\n",
    "1. Check the first 8 observations\n",
    "2. Check the dimensions of the dataset\n",
    "3. Print the information of `data` including the index dtype and column dtypes, non-null values and memory usage\n",
    "4. Generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the first 8 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 8 rows of the dataframe\n",
    "# To do that use the function head\n",
    "data.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the dimensions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions of training dataframe\n",
    "# Use the attribute .shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the information of `data` including the index dtype and column dtypes, non-null values and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get high-level information on the columns\n",
    "# use the .info() function\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function .describe()\n",
    "data.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook, we need to clean the data.\n",
    "We need to:\n",
    "1. Let's change the column names to something more meaningful\n",
    "2. Tidy the factors of the column gender - avoid redundancy\n",
    "3. Replace 1 and 0 with \"Yes\" and \"No\" in the `Decision` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's change the column names to something more meaningful\n",
    "Change the name of the columns `spent` and `salaRy` to `spent_month` and `salary`, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the column names\n",
    "# use the attribute .columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of the columns spent and salaRy using the function .rename()\n",
    "# use the parameter column to define the old and new names\n",
    "# use inplace = True\n",
    "data.rename(columns = {\"spent\":\"spent_month\", \"salaRy\":\"salary\"},\n",
    "            inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the column names\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tidy the factors of the column `gender` - avoid redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the column \"gender\"\n",
    "# Use the function .describe()\n",
    "data[\"gender\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the unique values of the column \"gender\"\n",
    "data[\"gender\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the redundant values of the column `gender` using only `Female` and `Male`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function .replace() on the column \"gender\"; replace female with Female\n",
    "data[\"gender\"] = data[\"gender\"].replace(\"female\", \"Female\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the unique values of the column \"gender\"\n",
    "data[\"gender\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function .replace() on the column \"gender\"; replace \"male\", \"M\", \"m\" with \"Male\"\n",
    "data[\"gender\"] = data[\"gender\"].replace([\"male\", \"M\", \"m\"], \"Male\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the unique values of the column \"gender\"\n",
    "data[\"gender\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace 1 and 0 with \"Yes\" and \"No\" in the `Decision` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function .replace() on the column 'Decision'; replace 1 and 0 with \"YES\" and \"NO\"\n",
    "data[\"Decision\"] = data[\"Decision\"].replace(1, \"YES\")\n",
    "data[\"Decision\"] = data[\"Decision\"].replace(0,\"NO\")\n",
    "data.info()\n",
    "data[\"Decision\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Spliting the dataset into NOPredict and Predict\n",
    "In this section of the notebook we need to:\n",
    "1. Drop all the empty values of the `Decision` column and save it as NOPredict\n",
    "2. Explore the data using boxplots and scatter plots of several variables in the y-axis and the decision on the x-axis\n",
    "3. Use the subset with all empty values in the column `Decision` and save it as Predict\n",
    "4. Divide the NOPredict subset into X and y, and then into train and test subsets for X and y\n",
    "5. Create the dummy variables to deal with categorical inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop all the empty values of the `Decision` column and save it as NOPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoPredict is the dataset with the known values for the decision\n",
    "# Use the function .dropna()\n",
    "NOPredict = data.dropna()\n",
    "NOPredict[\"Decision\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data using boxplots and scatter plots of several variables in the y-axis and the decision on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the NOPredict\n",
    "# Select for the y axis any variable and compare it with the decision column\n",
    "# Can you find a single variable that will help us to classify the decision column\n",
    "sns.boxplot(y='spent_month', x= 'Decision', data=NOPredict )\n",
    "plt.show()\n",
    "\n",
    "#sns.boxplot(y='No_jumpers_per_year', x= 'Decision', data=NOPredict )\n",
    "    \n",
    "\n",
    "sns.boxplot(y='Distance', x= 'Decision', data=NOPredict )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the NOPredict\n",
    "# Select for the x and y axis any variable and compare them with the decision column using the parameter hue = \"Decision\"\n",
    "# Can you find a single variable that will help us to classify the decision column\n",
    "sns.scatterplot(y='spent_month', x= 'Distance', hue = \"Decision\", data =NOPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(NOPredict , hue='Decision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the subset with all empty values in the column `Decision` and save it as Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function pd.isnull to subset the data with only null values\n",
    "Predict = data[pd.isnull(data[\"Decision\"])]\n",
    "Predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .describe to see a summary of Predict\n",
    "Predict.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the names of the columns first\n",
    "NOPredict.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the NOPredict subset into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection \n",
    "feature_cols = [\"age\", \"gender\", \"No_jumpers_per_year\", \"spent_today\", \"spent_month\",\n",
    "       \"salary\", \"Distance\", \"Online\"]\n",
    "X = NOPredict[feature_cols]\n",
    "y = NOPredict.Decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset X and y into X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset X and y using the function train_test_split\n",
    "# call the results X_train, X_test, y_train, y_test\n",
    "# use 75% for the train size\n",
    "# set the random seed to 246\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, \n",
    "                                                  test_size =.25,\n",
    "                                                  random_state = 123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dummy variables to deal with categorical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding all features in training set (X_train)\n",
    "X_train = pd.get_dummies(X_train )\n",
    "\n",
    "# One-hot encoding all features in testing set (X_test)\n",
    "X_test = pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Running the model \n",
    "Let's check the documentation of scikit-learn about decision trees https://scikit-learn.org/stable/modules/tree.html </br>\n",
    "Check out in particular section `1.10.5. Tips on practical use`\n",
    "\n",
    "Your facilitator will be walking through this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy model - no max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy = tree.DecisionTreeClassifier(\"entropy\" , random_state = 1234)\n",
    "clf_entropy.fit(X_train, y_train)\n",
    "y_pred = clf_entropy.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "clf_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf_entropy, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Entropy - no max depth\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "# print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity model - no max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = tree.DecisionTreeClassifier('gini', random_state = 1234)\n",
    "clf_gini.fit(X_train, y_train)\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "clf_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf_entropy, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Gini impurity model\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "#print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy model model - max_depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy_3 = tree.DecisionTreeClassifier(criterion='entropy', max_depth = 3, random_state = 1234)\n",
    "clf_entropy_3.fit(X_train, y_train)\n",
    "y_pred = clf_entropy_3.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "clf_entropy_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf_entropy_3, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Entropy model max depth 3\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "#print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini impurity  model - max depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini_3 = tree.DecisionTreeClassifier(criterion='gini', random_state = 1234, max_depth = 3)\n",
    "clf_gini_3.fit(X_train, y_train)\n",
    "y_pred = clf_gini_3.predict(X_test)\n",
    "y_pred = pd.Series(y_pred)\n",
    "clf_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf_gini_3, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names=X_train.columns,class_names = [\"NO\", \"YES\"])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gini impurity  model - max depth 3\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,y_pred))\n",
    "#print(\"Balanced accuracy:\", metrics.balanced_accuracy_score(y_test,y_pred))\n",
    "print('Precision score' , metrics.precision_score(y_test,y_pred, pos_label = \"YES\"))\n",
    "print('Recall score' , metrics.recall_score(y_test,y_pred, pos_label = \"NO\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model are you going to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now it is time to count how many loyal customers are going to buy the jumper\n",
    " 1. Let's calculate from the original dataset how many loyal customers said originally and explicitly that they will purchase the limited-edition jumper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Decision\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Let's calculate the number of people that according to the model will be willing to purchase the jumper </br>\n",
    "a. Subset the Predict dataset into `new_X` considering all the variables except `Decision` </br>\n",
    "b. Use that dataset to predict a new variable called `potential_buyers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection \n",
    "feature_cols = [\"age\", \"gender\", \"No_jumpers_per_year\", \"spent_today\", \"spent_month\",\n",
    "       \"salary\", \"Distance\", \"Online\"]\n",
    "new_X = Predict[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding all features in training set\n",
    "new_X = pd.get_dummies(new_X)\n",
    "potential_buyers = clf_gini.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(potential_buyers, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of potential buyers is 302 + 177 = 479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of interviewed people was\", data.salary.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the proportion of buyers\n",
    "buyer=479/701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Only \", round((buyer)*100, 2), \"% of people want to buy the limited edition jumper\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "As number if poeople who would buy is less than 70% the product will not be launched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use random forest tree to find the best fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
